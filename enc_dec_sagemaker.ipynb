{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ff6d3-5d83-4427-9194-13efcc3753d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU awscli boto3 sagemaker --quiet\n",
    "!pip install tritonclient[http] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d11bba-59eb-4663-bc55-bbf75882eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e69b79-4505-46dd-9c79-f56f5cf5861f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo amazon-linux-extras install epel -y\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196275b-f5ee-4c43-82b6-e4e77e06246c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=\"t5-small\"\n",
    "MODEL_TYPE=\"t5\"\n",
    "\n",
    "# For BART\n",
    "# MODEL_NAME=\"bart-base\"\n",
    "# MODEL_TYPE=\"bart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d315289-c782-401b-af7f-4087d3cf2d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792608e6-fbc7-4228-bb95-c42bf6bcccf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/google-t5/t5-small workspace/hf_models/$MODEL_NAME\n",
    "# !git clone git clone https://huggingface.co/facebook/bart-base workspace/hf_models/$MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a017b1e-b9eb-4036-ba84-2bf4aa1d5ae6",
   "metadata": {},
   "source": [
    "In the [generate_trtllm_triton_model_repo.sh](trtllm_backend_sagemaker/workspace/generate_trtllm_triton_model_repo.sh) script we build the TRT-LLM engine for encoder-decoder T5/BART model and prepare the Triton Model Repository. In this example we build TP Size=1 single_GPU engine with beam search (max beam width = 2), input len = 1024, output len = 200. To change this edit [generate_trtllm_triton_model_repo.sh](trtllm_backend_sagemaker/workspace/generate_trtllm_triton_model_repo.sh) script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286b856-423f-4d71-b617-29a2f2362695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRITON_IMAGE_URI=\"nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6f269-d3b7-45f0-bd0e-2f121bba6b9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus all --ulimit memlock=-1 --shm-size=12g -v ${PWD}/workspace:/workspace -w /workspace $TRITON_IMAGE_URI \\\n",
    "/bin/bash generate_trtllm_triton_model_repo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742b06d-fe2c-4bfc-bd8c-d3cf4df7ff84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116648dd-8c7a-4200-abdc-02b99c2935e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "triton_model_repo/\n",
    "├── ensemble\n",
    "│   ├── 1\n",
    "│   └── config.pbtxt\n",
    "├── postprocessing\n",
    "│   ├── 1\n",
    "│   │   └── model.py\n",
    "│   └── config.pbtxt\n",
    "├── preprocessing\n",
    "│   ├── 1\n",
    "│   │   └── model.py\n",
    "│   └── config.pbtxt\n",
    "└── tensorrt_llm\n",
    "    ├── 1\n",
    "    │   ├── engines\n",
    "    │   │   └── t5-small\n",
    "    │   │       ├── decoder\n",
    "    │   │       └── encoder\n",
    "    │   ├── hf_models\n",
    "    │   │   └── t5-small\n",
    "    │   │       ├── config.json\n",
    "    │   │       ├── flax_model.msgpack\n",
    "    │   │       ├── generation_config.json\n",
    "    │   │       ├── model.safetensors\n",
    "    │   │       ├── onnx\n",
    "    │   │       ├── pytorch_model.bin\n",
    "    │   │       ├── README.md\n",
    "    │   │       ├── rust_model.ot\n",
    "    │   │       ├── spiece.model\n",
    "    │   │       ├── tf_model.h5\n",
    "    │   │       ├── tokenizer_config.json\n",
    "    │   │       └── tokenizer.json\n",
    "    │   └── model.py\n",
    "    └── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f4151-10de-415b-8cd2-2b63c2ad9872",
   "metadata": {},
   "source": [
    "Next we push this image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739ef85-cb20-4d11-bc50-9acf9523c67b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker tag nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3 triton-trtllm\n",
    "!bash push_ecr.sh triton-trtllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757b359-6faf-43d9-9b49-918bae8322ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_image_uri = ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/triton-trtllm:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141042f-470a-4d28-8da6-499804c1d885",
   "metadata": {},
   "source": [
    "For a simple use case we will take the pre-trained NLP Bert model from Hugging Face and deploy it on SageMaker with Triton as the model server. The script for exporting this model can be found here. This is run as part of the generate_models.sh script from the previous cell. After the model is serialized we package it into the format that Triton and SageMaker expect it to be. We used the pre-configured config.pbtxt file provided with this repo here to specify model configuration which Triton uses to load the model. We tar the model directory and upload it to s3 to later create a SageMaker Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba30a7-f593-44a4-a71e-6bebccecc21f",
   "metadata": {},
   "source": [
    "## Packaging model files and uploading to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e254b-aa48-4bfc-9613-63363d8c5ce9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --exclude='.ipynb_checkpoints' --exclude='*.bin' \\\n",
    "--exclude='*.h5' --exclude='*.safetensors' --exclude=\"onnx\" \\\n",
    "--exclude='.git*' --exclude='.gitignore' --exclude='.gitattributes' --exclude='.gitmodules' \\\n",
    "-czvf model.tar.gz -C workspace/triton_model_repo/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af672b8-6334-44a2-b399-7babbe82cd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64577220-69ae-4302-ae53-21ea8fd39591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_uri = s3://sagemaker-us-east-1-ACCOUNTID/triton-trtllm-model/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016845b-a5c9-4eb9-921c-37a41eea3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-trtllm-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e78ea-2948-4f28-af83-1ac8f191dd55",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44e045-f374-4993-b4db-5510aa5ba179",
   "metadata": {},
   "source": [
    "We start off by creating a sagemaker model from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. SAGEMAKER_TRITON_DEFAULT_MODEL_NAME which specifies the name of the model to be loaded by Triton. The value of this key should match the folder name in the model package uploaded to s3. This variable is optional in case of a single model. In case of ensemble models, this key has to be specified for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT and SAGEMAKER_TRITON_THREAD_COUNT for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd82f1-e5bd-46f4-8817-75d5642895e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-trtllm-model-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e00f1-2ff6-482b-b91a-dbfb0d36d4ce",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d0705-09da-4765-a08f-cfe23b954e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-trtllm-model-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039338e-5a5f-4dc4-9fec-c5665fdd09e6",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9f543-8ca2-451b-9b63-dcd5ceea6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-trtllm-model-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3acf053-cf98-4fd5-a9b0-eea10eca3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c02ec7c-41f7-477c-8366-8acaee986703",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "Once we have the endpoint running we can use a sample text to do an inference using json as the payload format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571e0e8-d645-4662-9d66-5bf6cda1213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint_test(text_input, max_tokens,beam_width,temperature,repetition_penalty,min_length,bad_words,stop_words, endpoint_name): \n",
    "    payload = {}\n",
    "    payload[\"inputs\"] = [{\"name\" : \"text_input\", \"data\" : [text_input], \"datatype\" : \"BYTES\", \"shape\" : [1,1]},\n",
    "        {\"name\" : \"beam_width\", \"data\" : [beam_width], \"datatype\" : np_to_triton_dtype(np.int32), \"shape\" : [1,1]}, \n",
    "        {\"name\" : \"max_tokens\", \"data\" : [max_tokens], \"datatype\" : np_to_triton_dtype(np.int32), \"shape\" : [1,1]},\n",
    "        {\"name\" : \"temperature\", \"data\" : [temperature], \"datatype\" : np_to_triton_dtype(np.float32), \"shape\" : [1,1]},\n",
    "        {\"name\" : \"repetition_penalty\", \"data\" : [repetition_penalty], \"datatype\" : np_to_triton_dtype(np.float32), \"shape\" : [1,1]},\n",
    "        {\"name\" : \"min_length\", \"data\" : [min_length], \"datatype\" : np_to_triton_dtype(np.float32), \"shape\" : [1,1]},\n",
    "        {\"name\" : \"bad_words\", \"data\" : [bad_words], \"datatype\" : \"BYTES\", \"shape\" : [1,1]},\n",
    "        {\"name\" : \"stop_words\", \"data\" : [stop_words], \"datatype\" : \"BYTES\", \"shape\" : [1,1]},\n",
    "        ]\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    "    )\n",
    "    response_str = response[\"Body\"].read().decode()\n",
    "    json_object = json.loads(response_str)\n",
    "    return json_object['outputs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db148ac1-5d5b-4d3b-a239-3d084c650392",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519732f-fcd6-494f-8300-455282e011dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
